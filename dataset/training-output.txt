Generating full split: 
 227342/0 [00:08<00:00, 23593.86 examples/s]
tokenizer_config.json: 100%
 940/940 [00:00<00:00, 32.3kB/s]
config.json: 100%
 543/543 [00:00<00:00, 21.2kB/s]
vocab.json: 100%
 1.03M/1.03M [00:00<00:00, 10.6MB/s]
merges.txt: 100%
 595k/595k [00:00<00:00, 20.8MB/s]
special_tokens_map.json: 100%
 772/772 [00:00<00:00, 25.8kB/s]
Map: 100%
 181873/181873 [03:41<00:00, 681.00 examples/s]
Non-string content at index 797: <class 'NoneType'>
Non-string content at index 863: <class 'NoneType'>
Map: 100%
 22735/22735 [00:26<00:00, 936.64 examples/s]
Map: 100%
 22734/22734 [00:28<00:00, 898.24 examples/s]
model.safetensors: 100%
 498M/498M [00:03<00:00, 92.9MB/s]
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at mideind/IceBERT-igc and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
 [34104/34104 2:31:42, Epoch 3/3]
Step	Training Loss	Validation Loss	Accuracy	F1	Precision	Recall
500	0.254700	0.232465	0.913437	0.913487	0.914703	0.913437
1000	0.139600	0.202897	0.931823	0.931614	0.932242	0.931823
1500	0.261900	0.186632	0.938817	0.938610	0.940121	0.938817
2000	0.277000	0.153099	0.949065	0.948926	0.948883	0.949065
2500	0.213900	0.192857	0.938113	0.937930	0.942005	0.938113
3000	0.235400	0.292743	0.920739	0.918945	0.929295	0.920739
3500	0.101700	0.168713	0.948142	0.948081	0.948990	0.948142
4000	0.121300	0.182899	0.942775	0.942470	0.946691	0.942775
4500	0.156400	0.200844	0.944535	0.943992	0.946809	0.944535
5000	0.147100	0.152600	0.957159	0.956994	0.957100	0.957159
5500	0.085300	0.144973	0.959138	0.959176	0.959454	0.959138
6000	0.043100	0.193628	0.951704	0.951355	0.953333	0.951704
6500	0.113300	0.142481	0.958302	0.958272	0.959492	0.958302
7000	0.041300	0.216470	0.942907	0.942078	0.946527	0.942907
7500	0.091900	0.144267	0.959886	0.959897	0.959969	0.959886
8000	0.147000	0.170711	0.951001	0.950688	0.953700	0.951001
8500	0.125300	0.176682	0.955575	0.955341	0.957249	0.955575
9000	0.195700	0.126044	0.964108	0.963957	0.964245	0.964108
9500	0.108900	0.134620	0.964328	0.964144	0.964341	0.964328
10000	0.047400	0.128900	0.965692	0.965588	0.966255	0.965692
10500	0.124600	0.125242	0.967319	0.967287	0.967322	0.967319
11000	0.029600	0.139169	0.963009	0.962967	0.964112	0.963009
11500	0.044400	0.121771	0.969298	0.969282	0.969347	0.969298
12000	0.069200	0.147571	0.968023	0.967912	0.968359	0.968023
12500	0.037800	0.185832	0.959842	0.959686	0.961847	0.959842
13000	0.095200	0.184424	0.960765	0.960480	0.961023	0.960765
13500	0.051200	0.164926	0.966615	0.966505	0.967036	0.966615
14000	0.029600	0.127655	0.969870	0.969880	0.970129	0.969870
14500	0.069300	0.142999	0.969826	0.969872	0.970224	0.969826
15000	0.009100	0.138577	0.970354	0.970313	0.970414	0.970354
15500	0.006000	0.138110	0.970134	0.970051	0.970056	0.970134
16000	0.070300	0.188880	0.962217	0.962020	0.963419	0.962217
16500	0.077000	0.168511	0.964284	0.964036	0.965157	0.964284
17000	0.145100	0.135493	0.968111	0.967963	0.968406	0.968111
17500	0.081200	0.125820	0.972817	0.972791	0.972820	0.972817
18000	0.004000	0.136893	0.970574	0.970488	0.970542	0.970574
18500	0.061400	0.147252	0.968375	0.968250	0.968593	0.968375
19000	0.047900	0.174304	0.963448	0.963217	0.964579	0.963448
19500	0.002900	0.147012	0.968771	0.968679	0.968926	0.968771
20000	0.040700	0.120513	0.971718	0.971726	0.972160	0.971718
20500	0.056300	0.123861	0.972069	0.971983	0.972088	0.972069
21000	0.030300	0.128994	0.970926	0.970812	0.970892	0.970926
21500	0.017200	0.139171	0.970222	0.970180	0.970529	0.970222
22000	0.006900	0.151698	0.967715	0.967507	0.968127	0.967715
22500	0.063300	0.125537	0.973301	0.973262	0.973395	0.973301
23000	0.038400	0.155110	0.970002	0.969883	0.970437	0.970002
23500	0.014300	0.130626	0.975544	0.975509	0.975545	0.975544
24000	0.008100	0.153772	0.972729	0.972717	0.973169	0.972729
24500	0.001900	0.151687	0.972289	0.972197	0.972468	0.972289
25000	0.000700	0.169669	0.970706	0.970585	0.971103	0.970706
25500	0.017800	0.155893	0.972817	0.972768	0.972826	0.972817
26000	0.001300	0.144584	0.973961	0.973917	0.974058	0.973961
26500	0.002000	0.163530	0.971850	0.971745	0.972088	0.971850
27000	0.000200	0.150718	0.975764	0.975746	0.975762	0.975764
27500	0.072600	0.141562	0.974401	0.974368	0.974644	0.974401
28000	0.000400	0.174843	0.968595	0.968431	0.969440	0.968595
28500	0.000100	0.139025	0.975456	0.975444	0.975526	0.975456
29000	0.018400	0.159514	0.972289	0.972174	0.972528	0.972289
29500	0.000200	0.158971	0.973169	0.973092	0.973381	0.973169
30000	0.000300	0.154544	0.973917	0.973848	0.974091	0.973917
30500	0.000800	0.166553	0.971806	0.971707	0.972352	0.971806
31000	0.000100	0.177769	0.970178	0.970039	0.970924	0.970178
31500	0.000300	0.148488	0.974797	0.974729	0.974915	0.974797
32000	0.000100	0.148741	0.974137	0.974065	0.974373	0.974137
32500	0.000100	0.149279	0.973961	0.973900	0.974199	0.973961
33000	0.034700	0.154491	0.973477	0.973401	0.973725	0.973477
33500	0.000800	0.153606	0.973829	0.973778	0.974142	0.973829
34000	0.039200	0.151189	0.974005	0.973940	0.974238	0.974005
 [356/356 01:44]
Validation Results: {'eval_loss': 0.15068301558494568, 'eval_accuracy': 0.9742247635803827, 'eval_f1': 0.9741619186907529, 'eval_precision': 0.9744410129945318, 'eval_recall': 0.9742247635803827, 'eval_runtime': 52.2635, 'eval_samples_per_second': 435.008, 'eval_steps_per_second': 6.812, 'epoch': 3.0}
Test Set Evaluation Results: {'eval_loss': 0.13586454093456268, 'eval_accuracy': 0.9778745491334565, 'eval_f1': 0.9778175828416871, 'eval_precision': 0.9780436726704461, 'eval_recall': 0.9778745491334565, 'eval_runtime': 52.3172, 'eval_samples_per_second': 434.541, 'eval_steps_per_second': 6.805, 'epoch': 3.0}
('models/IceBert/tokenizer_config.json',
 'models/IceBert/special_tokens_map.json',
 'models/IceBert/vocab.json',
 'models/IceBert/merges.txt',
 'models/IceBert/added_tokens.json',
 'models/IceBert/tokenizer.json')